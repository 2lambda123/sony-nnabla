{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed Precision Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, for training a neural network, we used to use `FP32` for weights and activations; however computation costs for trainig a neural network rapidly increase over years as the success of deep learning and the growing size of a neural nework. It indiates that we need to spend much more time for training a huge size of a neural network while we would like to do lots of trials before a product launch. To address this problem, companys (e.g., NVIDIA) introduced an accelarator for speeding up computation. For example, NVIDIA Volta has [Tensor Cores](https://devblogs.nvidia.com/programming-tensor-cores-cuda-9/) to speed up computation.\n",
    "\n",
    "However, it uses `FP16` weights, activations, gradients, and the range of `FP16` is very limited when compared to that of `FP32`, meaning that sometimes (or often) values of gradients overflow and/or underflow, which affects the performance of a neural network or makes it collapse during training.\n",
    "\n",
    "Mixed precision training is one of the algorithms to circumvent that problem while maintaining the same results that we could obtain with `FP32` networks. It is well-described in [The Training with Mixed Precision User Guide](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html) and [Mixed Precision Training](https://arxiv.org/abs/1710.03740).\n",
    "\n",
    "This tutorial explains how to do the mixed precision training in NNabla step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the mixed precision training are composed of three parts.\n",
    "\n",
    "1. Use the accelarator for computation (here we assume Tensor Cores)\n",
    "2. Use loss scaling to prevent underflow\n",
    "3. Use dynamic loss caling to prevent overflow/underflow\n",
    "\n",
    "In NNabla, we can do the correspondinces as follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use Tensor Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = get_extension_context(\"cudnn\", type_config=\"half\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use loss scaling to prevent underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_scale = 8\n",
    "loss.backward(loss_scale)\n",
    "solver.scale_grad(1. / loss_scale)  # do some graident clipping, etc. after this\n",
    "solver.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use dynamic loss scaling to prevent overflow/underflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_scale = 8\n",
    "scaling_factor = 2\n",
    "counter = 0\n",
    "interval = 2000\n",
    "...\n",
    "loss.backward(loss_scale, ...)\n",
    "...\n",
    "if solver.check_inf_or_nan_grad():\n",
    "    loss_scale /= scaling_factor\n",
    "    counter = 0\n",
    "else:\n",
    "    solver.scale_grad(1. / loss_scale) # do some graident clipping, etc. after this\n",
    "    solver.update()\n",
    "    if counter > interval:\n",
    "        loss_scale *= scaling_factor\n",
    "        counter = 0\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__ that currently the procedures of 2nd (Use loss scaling to prevent underflow) and 3rd (Use loss scaling to prevent overflow) are exprimental, and we are now trying to speed up the mixed precision training, so API might change for future use, especially 3rd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All-in-one Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous step-by-step example, the 3rd step is lengthy in a training loop, thus we can write a wrapper class like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoLossScalingUpdater(object):\n",
    "\n",
    "    def __init___(self, solver, loss,\n",
    "                  scale=8, scaling_factor=2, N=2000, clear_buffer=True,\n",
    "                  accum_grad=1,\n",
    "                  comm=None,\n",
    "                  grads=[]):\n",
    "        self.solver = solver\n",
    "        self.loss = loss\n",
    "        self.scale = scale\n",
    "        self.N = N\n",
    "        self.clear_buffer = clear_buffer\n",
    "        self.accum_grad = accum_grad\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.comm = comm\n",
    "        self.grads = grads\n",
    "        self.counter = 0\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        # Forward and backward\n",
    "        for _ in range(self.accum_grad):\n",
    "            # forward\n",
    "            self.loss.forward(clear_no_need_grad=self.clear_buffer)\n",
    "\n",
    "            # backward with scale\n",
    "            self.loss.backward(self.scale, clear_buffer=self.clear_buffer)\n",
    "\n",
    "        # AllReduce\n",
    "        if self.comm and len(self.grads) != 0:\n",
    "            self.comm.all_reduce(self.grads, division=False, inplace=False)\n",
    "\n",
    "        # Check Inf/NaN in grads\n",
    "        if self.solver.check_inf_or_nan_grad():\n",
    "            self.scale /= self.scaling_factor\n",
    "            self.counter = 0\n",
    "            return\n",
    "\n",
    "        # Rescale grads\n",
    "        self.solver.scale_grad(1. / self.scale)\n",
    "\n",
    "        # Do some graident clipping, etc.\n",
    "\n",
    "        # Update\n",
    "        self.solver.update()\n",
    "        if self.counter > self.N:\n",
    "            self.scale *= self.scaling_factor\n",
    "            self.conter = 0\n",
    "        self.counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, call the update method in a training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solver = <Solver>\n",
    "loss = <Loss Variable of Network>\n",
    "updater = AutoLossScalingUpdater(solver, loss)\n",
    "...\n",
    "x.d = <data>\n",
    "y.d = <label>\n",
    "...\n",
    "updater.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the mixed-precision training, the followings are premise:\n",
    "\n",
    "1. Solver contains `FP16` weights and the `FP32` copy of weights. Solvers in NNabla hold `FP32` weights and weight gradients and cast it to `FP16` weights in forward pass and to `FP16` weight gradients in backward pass if one sets `type_config=\"half\"`.\n",
    "\n",
    "2. Reductions should be left in `FP32`, for examples, the statistics (mean and variance) computed by the batch-normalization, Mean, Sum, SoftMax, SoftMaxCrossEntropy, etc. (see [The Training with Mixed Precision User Guide](https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html)). In NNabla, these functions are automatically fallbacked to use `FP32`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
