model:
  in_channels: 3
  num_hidden: 256
  num_res_layers: 2
  embedding_dim: 256
  num_embeddings: 128
  commitment_cost: 1
  saved_models_dir: tmp.monitor_cifar10/trained_models_cifar10/
  checkpoint: None
  rng: 313
  decay: 0.99

pixelcnn:
  in_channels: 128
  out_channels: 128
  num_features: 64
  num_layers: 15
  conditional: True
  num_classes: 10
  saved_models_dir: trained_prior/
  base_model_checkpoint: trained_models_cifar10/epoch_29

train:
  batch_size: 32
  num_training_updates: 25000 
  learning_rate: 2.0e-4
  weight_decay: 0
  decay: 0.99 # For VQ with EMA (to be implemented later)
  num_epochs: 40
  save_param_step_interval: 5
  logger_step_interval: 100
  learning_rate_decay_epochs: []
  learning_rate_decay_factor: 1
  solver: adam

val:
  batch_size: 32
  interval: 1

monitor:
  path: tmp.monitor_cifar10/
  train_loss: Training-loss-cifar10 
  train_recon: Training-reconstructions-cifar10
  val_loss: Validation-loss-cifar10 
  val_recon: Validation-reconstructions-cifar10

dataset:
  name: cifar10
  train_size: 50000
  val_size: 10000
  with_memory_cache: False
  with_file_cache: False
  shuffle: True


extension_module: cudnn
device_id: '0'
